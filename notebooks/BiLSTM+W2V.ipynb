{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091a5ae0",
   "metadata": {},
   "source": [
    "# У меня очистились все аутпуты\n",
    "Но весь код должен запускаться, если есть _keras_, _tensorflow_ и _gensim_. _Requirements_ не делал, потому что в итоге в прод это не пошло."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ec083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.metrics import AUC\n",
    "import tensorflow as tf\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import List, Tuple\n",
    "from functools import partial\n",
    "from numba import jit\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9061c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = '../data/clear_train.csv'\n",
    "val_clean = '../data/clear_val.csv'\n",
    "\n",
    "train_dirty = '../data/train.csv'\n",
    "val_dirty = '../data/val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import perf_counter\n",
    "\n",
    "def timing(function):\n",
    "    \"\"\"Time measuring decorator.\"\"\"\n",
    "    @wraps(function)\n",
    "    def wrap(*args, **kwargs):\n",
    "        \"\"\"Wrap function for the timing decorator.\"\"\"\n",
    "        time_start = perf_counter()\n",
    "        function_result = function(*args, **kwargs)\n",
    "        time_end = perf_counter()\n",
    "        print(\n",
    "            \"func {a} took: {c:0.5f}s.\".format(\n",
    "                a=function.__name__,\n",
    "                c=time_end - time_start,\n",
    "            )\n",
    "        )\n",
    "        return function_result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_maker:\n",
    "    def __init__(self,\n",
    "                 train_path: str,\n",
    "                 val_path: str,\n",
    "                 emb_dims: int,\n",
    "                 min_words_count: int,\n",
    "                 window_size: int,\n",
    "                 max_len_tokenizer: int,\n",
    "                 lstm_epochs: int,\n",
    "                 lstm_batch_size: int,         \n",
    "        ):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.emb_dims = emb_dims\n",
    "        self.min_words_count = min_words_count\n",
    "        self.window_size = window_size\n",
    "        self.max_len_tokenizer = max_len_tokenizer\n",
    "        self.lstm_epochs = lstm_epochs\n",
    "        self.lstm_batch_size = lstm_batch_size\n",
    "\n",
    "    @timing\n",
    "    def get_dataset(self, path: str) -> Tuple[List, pd.DataFrame]:\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        descriptions = df['description'].fillna(\"\").values\n",
    "        lines = descriptions.tolist()\n",
    "        description = Parallel(n_jobs=cpu_count())(delayed(lambda x: str(x).split())(x) for x in lines)\n",
    "        return df[\"is_bad\"], description\n",
    "\n",
    "    @timing\n",
    "    def train_word2vec(self, data, path: str):\n",
    "        self.w2v = Word2Vec(data, size=self.emb_dims, min_count=self.min_words_count, window=self.window_size)\n",
    "        self.w2v.save(path)\n",
    "\n",
    "    def load_word2vec(self, path: str):\n",
    "        self.w2v = Word2Vec.load(path)\n",
    "\n",
    "    def tokenizer_fit_on_train(self):\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_len_tokenizer, lower=False)\n",
    "        self.tokenizer.fit_on_texts(self.data_train)\n",
    "\n",
    "    @timing\n",
    "    def tokenize_dataset(self, dataset):\n",
    "        sequences = self.tokenizer.texts_to_sequences(dataset)\n",
    "        return pad_sequences(sequences, maxlen=self.max_len_tokenizer)\n",
    "\n",
    "    def create_embedding_matrix(self):\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, self.emb_dims))\n",
    "        \n",
    "        for word, i in tqdm(self.tokenizer.word_index.items()):\n",
    "            embedding_vector = self.w2v.wv[word]\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    @timing\n",
    "    def create_BiLSTM(self):\n",
    "        self.BiLSTM = Sequential()\n",
    "        self.BiLSTM.add(Embedding(input_dim=self.vocab_size, output_dim=self.emb_dims, \n",
    "                                        input_length=self.max_len_tokenizer, weights = [self.embedding_matrix]))\n",
    "        self.BiLSTM.add(Bidirectional(LSTM(64, dropout=0.25, recurrent_dropout=0.1)))\n",
    "        self.BiLSTM.add(Dense(10))\n",
    "        self.BiLSTM.add(Dropout(0.3))\n",
    "        self.BiLSTM.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        self.learning_rate_reduction = ReduceLROnPlateau(monitor=\"val_auc\", \n",
    "                                                         patience=3, \n",
    "                                                         verbose=1, \n",
    "                                                         factor=0.5, \n",
    "                                                         min_lr=0.00001,\n",
    "                                                         )\n",
    "        self.BiLSTM.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=AUC())\n",
    "        del self.embedding_matrix\n",
    "\n",
    "    @timing\n",
    "    def train_BiLSTM(self, path: str):\n",
    "        checkpoint = ModelCheckpoint(path,\n",
    "                                     monitor=\"val_auc\",\n",
    "                                     verbose=1,\n",
    "                                     save_best_only=True,\n",
    "                                     mode='auto',\n",
    "                                     period=1,\n",
    "        )\n",
    "        self.BiLSTM.fit(self.data_train,\n",
    "                        self.y_train,\n",
    "                        epochs=self.lstm_epochs,\n",
    "                        batch_size=self.lstm_batch_size,\n",
    "                        validation_data=(self.data_val, self.y_val),\n",
    "                        callbacks=[self.learning_rate_reduction, checkpoint],\n",
    "        )\n",
    "        \n",
    "        self.BiLSTM.save(path)\n",
    "\n",
    "    def load_BiLSTM(self, path: str):\n",
    "        self.BiLSTM = keras.models.load_model(path)\n",
    "\n",
    "    @timing\n",
    "    def text_saver(self):\n",
    "        train = pd.read_csv(self.train_path)\n",
    "        train[\"LSTM_result\"] = self.BiLSTM.predict_proba(self.data_train)\n",
    "        train.to_csv(self.train_path)\n",
    "        del train\n",
    "        \n",
    "        val = pd.read_csv(self.val_path)\n",
    "        val[\"LSTM_result\"] = self.BiLSTM.predict_proba(self.data_val)\n",
    "        val.to_csv(self.val_path)\n",
    "        del val\n",
    "\n",
    "    @timing\n",
    "    def process(self, preload_w2v: bool, w2v_path: str, preload_lstm: bool, lstm_path: str, save_text: bool):\n",
    "        self.y_train, self.data_train = self.get_dataset(self.train_path)\n",
    "        self.y_val, self.data_val = self.get_dataset(self.val_path)\n",
    "        print(\"datasets loaded\")\n",
    "\n",
    "        if not preload_w2v:\n",
    "            self.train_word2vec(self.data_train, w2v_path)\n",
    "        else:\n",
    "            self.load_word2vec(w2v_path)\n",
    "        print(\"w2v loaded\")\n",
    "        \n",
    "        self.tokenizer_fit_on_train()\n",
    "        self.data_train = self.tokenize_dataset(self.data_train)\n",
    "        self.data_val = self.tokenize_dataset(self.data_val)\n",
    "        print(\"texts tokenized\")\n",
    "        \n",
    "        self.create_embedding_matrix()\n",
    "        self.create_BiLSTM()\n",
    "        if not preload_lstm:\n",
    "            self.train_BiLSTM(lstm_path)\n",
    "        else:\n",
    "            self.load_BiLSTM(lstm_path)\n",
    "        print(\"BiLSTM loaded\")\n",
    "        \n",
    "        if save_text:\n",
    "            self.text_saver()\n",
    "            print(\"text saved\")\n",
    "        print(\"end\")\n",
    "    \n",
    "    def __del__(self):\n",
    "        del self.train_path\n",
    "        del self.val_path\n",
    "        del self.emb_dims\n",
    "        del self.min_words_count\n",
    "        del self.window_size\n",
    "        del self.max_len_tokenizer\n",
    "        del self.lstm_epochs\n",
    "        del self.lstm_batch_size\n",
    "        del self.learning_rate_reduction\n",
    "        del self.BiLSTM\n",
    "        del self.y_train\n",
    "        del self.data_train\n",
    "        del self.y_val\n",
    "        del self.data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "clean_BiLSTM = BiLSTM_maker(train_clean, val_clean, 128, 200000, 5, 100, 4, 1000)\n",
    "clean_BiLSTM.process(True, \"../lib/word2vec/word2vec_clean.model\", True, \"../lib/lstm/lstm_clean.h5\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b12143",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_val_bilstm = clean_BiLSTM.BiLSTM.predict(clean_BiLSTM.data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(clean_BiLSTM.y_val, y_val_bilstm)\n",
    "print('ROC_AUC score for val data using LogReg:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4225928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_printer(X_train, X_val, y_train, y_val, model):\n",
    "    headers = ['Model', 'Dataset', 'Metric', 'Value']\n",
    "    table = []\n",
    "    model_name = type(model).__name__\n",
    "    for dataset, ds_type, labels in zip([X_train, X_val], [\"train\", \"val\"], [y_train, y_val]):\n",
    "        accuracy = accuracy_score(labels, model.predict(dataset))\n",
    "        labels_pred = model.predict_proba(dataset)[:, 1]\n",
    "        rocauc = roc_auc_score(labels, labels_pred)\n",
    "        table.append([model_name, ds_type, \"accuracy\", accuracy])\n",
    "        table.append([model_name, ds_type, \"AUC\", rocauc])\n",
    "    print(tabulate(table, headers=headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44446abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_BiLSTM = BiLSTM(train_dirty, val_dirty)\n",
    "dirty_BiLSTM.process(False, \"../lib/word2vec/word2vec_dirty.model\", False, \"../lib/lstm/lstm_dirty.h5\")\n",
    "del dirty_BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17594378",
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8fa99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
