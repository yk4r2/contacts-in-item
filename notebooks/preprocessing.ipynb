{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e389feb",
   "metadata": {},
   "source": [
    "## Импортики, ничего интересного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda39bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from functools import lru_cache, partial\n",
    "from json import load as json_load\n",
    "\n",
    "from nltk import download as nltk_download\n",
    "from nltk.corpus import stopwords\n",
    "from pandarallel import pandarallel\n",
    "from pandas import DataFrame, concat, read_csv\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# cores_count = os.popen(\"grep -m 1 'cpu cores' /proc/cpuinfo\").read().split()[-1]\n",
    "# pandarallel.initialize(progress_bar=False, nb_workers=int(cores_count))\n",
    "pandarallel.initialize(progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdd586",
   "metadata": {},
   "source": [
    "## Лемматизация и клининг регулярками\n",
    "\n",
    "Для начала напишем лемматизацию и клинер, воспользовавшись «знаменитым» списком стоп-слов от Артемия Лебедева.\n",
    "\n",
    "На самом деле, я его добавил после того, как обнаружил при обучении LSTM, что неправильно обработал данные, на всякий случай, но лучше пусть он будет, чем его не будет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0cbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yk4r2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_download('stopwords')\n",
    "\n",
    "with open('../lib/models/stopwords.json') as json_file:\n",
    "    additional_stopwords = json_load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "my_stopwords = set(stopwords.words('russian'))\n",
    "my_stopwords.update(additional_stopwords)\n",
    "my_stopwords.update(stopwords.words('english'))\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a651fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regexp_replacer(cyrillic: str, symbol: str, string: str) -> str:\n",
    "    string = re.sub(cyrillic, symbol, string)\n",
    "    return string\n",
    "\n",
    "def clean(string: str) -> str:\n",
    "    string = regexp_replacer(r'[^0-9a-zA-Zа-яА-ЯёЁ\\.,\\(\\)]+', ' ', string)\n",
    "    string = regexp_replacer(r'([^\\w ])', r' \\1', string)\n",
    "    string = regexp_replacer(r'([^ \\w])', r'\\1', string)\n",
    "    string = regexp_replacer(r' +', r' ', string)\n",
    "    string = regexp_replacer(r'^ ', r'', string)\n",
    "    string = regexp_replacer(r'[\\W_]+', ' ', string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "def find_from_dict(searcher: dict, string: str) -> list:\n",
    "    occurrencies = []\n",
    "    for name, regexp in searcher.items():\n",
    "         occurrencies.append(int(bool(re.search(regexp, string))))\n",
    "    return occurrencies\n",
    "\n",
    "def replace_from_dict(replacer: dict, string: str) -> str:\n",
    "    for cyrillic, symbol in replacer.items():\n",
    "        string = regexp_replacer(cyrillic, str(symbol), string)\n",
    "    return string\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def lemmatizer(word: str, morph) -> list:\n",
    "    return morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8876b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text: str) -> list:\n",
    "    text = clean(str(text)).split()\n",
    "    text = [\n",
    "        word for word in text\n",
    "        if word not in my_stopwords\n",
    "    ]\n",
    "    return ' '.join(map(lambda word: lemmatizer(word, morph), text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce273a84",
   "metadata": {},
   "source": [
    "## Очистка датасетов и поиск контактов регулярочками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d112de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 430 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_val = DataFrame()\n",
    "df_train = DataFrame()\n",
    "\n",
    "for chunk in read_csv('../data/val.csv', chunksize=100000):\n",
    "    df_val = concat([df_val, chunk])\n",
    "for chunk in read_csv('../data/train.csv', chunksize=100000):\n",
    "    df_train = concat([df_train, chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b10cdd",
   "metadata": {},
   "source": [
    "Собственно, регулярочки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11009585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.34 s, sys: 6 s, total: 11.3 s\n",
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('../lib/models/regexps/regexp.json') as json_file:\n",
    "    regexps = json_load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "our_find_from_dict = partial(find_from_dict, regexps)\n",
    "regexps_train = df_train['description'].parallel_apply(lambda string: our_find_from_dict(string))\n",
    "regexps_val = df_val['description'].parallel_apply(lambda string: our_find_from_dict(string))\n",
    "regexps_train = DataFrame((item for item in regexps_train), columns = regexps.keys())\n",
    "regexps_val = DataFrame((item for item in regexps_val), columns = regexps.keys())\n",
    "\n",
    "df_val.description = df_val['description'].fillna('').parallel_apply(clean)\n",
    "df_val.title = df_val['title'].fillna('').parallel_apply(clean)\n",
    "df_train.description = df_train['description'].fillna('').parallel_apply(clean)\n",
    "df_train.title = df_train['title'].fillna('').parallel_apply(clean)\n",
    "\n",
    "df_train = concat([df_train, regexps_train], axis=1)\n",
    "df_val = concat([df_val, regexps_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e1495",
   "metadata": {},
   "source": [
    "Тут я заменяю слова типа \"собака\", \"точка\" и подобные на нормальные аналоги, чтобы проще искать их потом.\n",
    "\n",
    "Я знаю, что это можно было более оптимально написать как функцию clean, но я уже 4 часа обрабатываю датасет, поэтому пусть будет так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d5f3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../lib/models/regexps/punctuation.json') as f:\n",
    "    punctuation = json_load(f)\n",
    "    f.close()\n",
    "\n",
    "df_val.description = df_val['description'].parallel_apply(lambda string: replace_from_dict(punctuation, string))\n",
    "df_val.title = df_val['title'].parallel_apply(lambda string: replace_from_dict(punctuation, string))\n",
    "df_train.description = df_train['description'].parallel_apply(lambda string: replace_from_dict(punctuation, string))\n",
    "df_train.title = df_train['title'].parallel_apply(lambda string: replace_from_dict(punctuation, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aef3fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           31Gi        17Gi       8,9Gi        31Mi       4,4Gi        12Gi\r\n",
      "Swap:         2,0Gi       674Mi       1,3Gi\r\n"
     ]
    }
   ],
   "source": [
    "!free -mh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45cac2",
   "metadata": {},
   "source": [
    "Лемматизация и клининг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eefd2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val done!\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb5df616e4142dc95a404f62f830dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=61531), Label(value='0 / 61531')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "CPU times: user 7.56 s, sys: 4.93 s, total: 12.5 s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_val['description'] = df_val.description.parallel_apply(process_text)\n",
    "df_val['title'] = df_val.title.parallel_apply(process_text)\n",
    "print('val done!')\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "df_train['description'] = df_train.description.parallel_apply(process_text)\n",
    "pandarallel.initialize(progress_bar=False)\n",
    "df_train['title'] = df_train.title.parallel_apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac6a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'девочка белый девочка белый цена документ документ дорогой 200 миникнуть привить подрастать беленький короткий носик очень пушистый ласковый дополнительный инфо тело фото наш доставка мочь организовать'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[457437].title + ' ' + df_train.iloc[457437].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99efb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['title_and_description'] = df_train.title + ' ' + df_train.description\n",
    "df_val['title_and_description'] = df_val.title + ' ' + df_val.description\n",
    "\n",
    "df_train.drop(['title', 'description'], axis=1, inplace=True)\n",
    "df_val.drop(['title', 'description'], axis=1, inplace=True)\n",
    "df_train.drop(['subcategory', 'price', 'region', 'city', 'datetime_submitted'], axis=1, inplace=True)\n",
    "df_train.drop(['phone_normal', 'youtube', 'site'], axis=1, inplace=True)\n",
    "df_val.drop(['subcategory', 'price', 'region', 'city', 'datetime_submitted'], axis=1, inplace=True)\n",
    "df_val.drop(['phone_normal', 'youtube', 'site'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5653c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.title_and_description.parallel_apply(lambda string: re.sub('[^A-Za-z0-9\\.\\@\\ \\-\\_]', ' ', string))\n",
    "df_train['text'] = df_train['text'].parallel_apply(lambda string: re.sub(' +', ' ', string))\n",
    "\n",
    "df_train['numbers'] = df_train.title_and_description.parallel_apply(lambda string: re.sub('[^0-9\\+\\(\\)\\-]', ' ', string))\n",
    "df_train['numbers'] = df_train['numbers'].parallel_apply(lambda string: re.sub(' +', ' ', string))\n",
    "\n",
    "df_val['text'] = df_val.title_and_description.parallel_apply(lambda string: re.sub('[^A-Za-z0-9\\.\\@\\ \\-\\_]', ' ', string))\n",
    "df_val['text'] = df_val['text'].parallel_apply(lambda string: re.sub(' +', ' ', string))\n",
    "\n",
    "df_val['numbers'] = df_val.title_and_description.parallel_apply(lambda string: re.sub('[^0-9\\+\\(\\)\\-]', ' ', string))\n",
    "df_val['numbers'] = df_val['numbers'].parallel_apply(lambda string: re.sub(' +', ' ', string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c64f3",
   "metadata": {},
   "source": [
    "## Сохранение датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/train_preprocessed.csv', index=False)\n",
    "df_val.to_csv('../data/val_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37682b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
