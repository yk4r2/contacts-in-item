{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e389feb",
   "metadata": {},
   "source": [
    "## Импортики, ничего интересного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fda39bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from functools import lru_cache, partial\n",
    "from json import load as json_load\n",
    "\n",
    "from nltk import download as nltk_download\n",
    "from nltk.corpus import stopwords\n",
    "from pandarallel import pandarallel\n",
    "from pandas import DataFrame, concat, read_csv\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# cores_count = os.popen(\"grep -m 1 'cpu cores' /proc/cpuinfo\").read().split()[-1]\n",
    "# pandarallel.initialize(progress_bar=False, nb_workers=int(cores_count))\n",
    "pandarallel.initialize(progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdd586",
   "metadata": {},
   "source": [
    "## Лемматизация и клининг регулярками\n",
    "\n",
    "Для начала напишем лемматизацию и клинер, воспользовавшись «знаменитым» списком стоп-слов от Артемия Лебедева.\n",
    "\n",
    "На самом деле, я его добавил после того, как обнаружил при обучении LSTM, что неправильно обработал данные, на всякий случай, но лучше пусть он будет, чем его не будет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e0cbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yk4r2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_download('stopwords')\n",
    "\n",
    "with open('../lib/models/stopwords.json') as json_file:\n",
    "    additional_stopwords = json_load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "my_stopwords = set(stopwords.words('russian'))\n",
    "my_stopwords.update(additional_stopwords)\n",
    "my_stopwords.update(stopwords.words('english'))\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a651fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regexp_replacer(cyrillic: str, symbol: str, string: str) -> str:\n",
    "    string = re.sub(cyrillic, symbol, string)\n",
    "    return string\n",
    "\n",
    "def clean(string: str) -> str:\n",
    "    string = regexp_replacer(r'[^0-9a-zA-Zа-яА-ЯёЁ\\.,\\(\\)]+', ' ', string)\n",
    "    string = regexp_replacer(r'([^\\w ])', r' \\1', string)\n",
    "    string = regexp_replacer(r'([^ \\w])', r'\\1', string)\n",
    "    string = regexp_replacer(r' +', r' ', string)\n",
    "    string = regexp_replacer(r'^ ', r'', string)\n",
    "    string = regexp_replacer(r'[\\W_]+', ' ', string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "def find_from_dict(searcher: dict, string: str) -> list:\n",
    "    occurrencies = []\n",
    "    for name, regexp in searcher.items():\n",
    "         occurrencies.append(int(bool(re.search(regexp, string))))\n",
    "    return occurrencies\n",
    "\n",
    "def replace_from_dict(replacer: dict, string: str) -> str:\n",
    "    for cyrillic, symbol in replacer.items():\n",
    "        string = regexp_replacer(cyrillic, str(symbol), string)\n",
    "    return string\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def lemmatizer(word: str, morph) -> list:\n",
    "    return morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f8876b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text: str) -> list:\n",
    "    text = clean(str(text)).split()\n",
    "    text = [\n",
    "        word for word in text\n",
    "        if word not in russian_stopwords\n",
    "        and word not in english_stopwords\n",
    "    ]\n",
    "    return ' '.join(map(lambda word: lemmatizer(word, morph), text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce273a84",
   "metadata": {},
   "source": [
    "## Очистка датасетов и поиск контактов регулярочками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d112de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.99 s, sys: 267 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_val = DataFrame()\n",
    "df_train = DataFrame()\n",
    "\n",
    "for chunk in read_csv('../data/val.csv', chunksize=100000):\n",
    "    df_val = concat([df_val, chunk])\n",
    "for chunk in read_csv('../data/train.csv', chunksize=100000):\n",
    "    df_train = concat([df_train, chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b10cdd",
   "metadata": {},
   "source": [
    "Собственно, регулярочки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11009585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.23 s, sys: 4.76 s, total: 9.99 s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('../lib/models/regexps/regexp.json') as json_file:\n",
    "    regexps = json_load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "our_find_from_dict = partial(find_from_dict, regexps)\n",
    "regexps_train = df_train['description'].parallel_apply(lambda string: our_find_from_dict(string))\n",
    "regexps_val = df_val['description'].parallel_apply(lambda string: our_find_from_dict(string))\n",
    "regexps_train = DataFrame((item for item in regexps_train), columns = regexps.keys())\n",
    "regexps_val = DataFrame((item for item in regexps_val), columns = regexps.keys())\n",
    "\n",
    "df_val.description = df_val['description'].fillna('').parallel_apply(clean)\n",
    "df_val.title = df_val['title'].fillna('').parallel_apply(clean)\n",
    "df_train.description = df_train['description'].fillna('').parallel_apply(clean)\n",
    "df_train.title = df_train['title'].fillna('').parallel_apply(clean)\n",
    "\n",
    "df_train = concat([df_train, regexps_train], axis=1)\n",
    "df_val = concat([df_val, regexps_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e1495",
   "metadata": {},
   "source": [
    "Тут я заменяю слова типа \"собака\", \"точка\" и подобные на нормальные аналоги, чтобы проще искать их потом.\n",
    "\n",
    "Я знаю, что это можно было более оптимально написать как функцию clean, но я уже 4 часа обрабатываю датасет, поэтому пусть будет так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d5f3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../lib/models/regexps/numbers_and_punctuation.json') as f:\n",
    "    punctuation = json_load(f)\n",
    "    f.close()\n",
    "\n",
    "df_val.description = df_val['description'].parallel_apply(lambda string: replace_from_dict(punctuation, string))\n",
    "df_val.title = df_val['title'].parallel_apply(lambda string: replace_from_dict(punctuation, string))\n",
    "df_train.description = df_train['description'].parallel_apply(lambda string: replace_from_dict(punctuation, string))\n",
    "df_train.title = df_train['title'].parallel_apply(lambda string: replace_from_dict(punctuation, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aef3fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           31Gi        10Gi       9,8Gi        11Mi        10Gi        19Gi\r\n",
      "Swap:         2,0Gi       6,0Mi       2,0Gi\r\n"
     ]
    }
   ],
   "source": [
    "!free -mh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45cac2",
   "metadata": {},
   "source": [
    "Лемматизация и клининг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eefd2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val done!\n",
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c7165f095247f1b602584f97e6b73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=61531), Label(value='0 / 61531')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "CPU times: user 7.55 s, sys: 3.58 s, total: 11.1 s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_val['description'] = df_val.description.parallel_apply(process_text)\n",
    "df_val['title'] = df_val.title.parallel_apply(process_text)\n",
    "print('val done!')\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "df_train['description'] = df_train.description.parallel_apply(process_text)\n",
    "pandarallel.initialize(progress_bar=False)\n",
    "df_train['title'] = df_train.title.parallel_apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac6a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'девочка белый девочка белый цена документ документ дорогой 200 миникнуть привить подрастать беленький короткий носик очень пушистый ласковый дополнительный инфо тело фото наш доставка мочь организовать'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[457437].title + ' ' + df_train.iloc[457437].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99efb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# А в скале, например, так сделать нельзя, потому что статическая типизация лучше динамической.\n",
    "\n",
    "df_train['title_and_description'] = df_train.title + ' ' + df_train.description\n",
    "df_val['title_and_description'] = df_val.title + ' ' + df_val.description\n",
    "df_train.drop(['title', 'description'], axis=1, inplace=True)\n",
    "df_val.drop(['title', 'description'], axis=1, inplace=True)\n",
    "df_train.drop(['subcategory', 'price', 'region', 'city', 'datetime_submitted'], axis=1, inplace=True)\n",
    "df_val.drop(['subcategory', 'price', 'region', 'city', 'datetime_submitted'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e020860c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'is_bad', 'phone_normal', 'phone_biased', 'email',\n",
       "       'youtube', 'home_phone', 'site', 'phone_operators',\n",
       "       'title_and_description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca9ab4",
   "metadata": {},
   "source": [
    "## Вот так я \"отбирал\" регулярки и прочие полезности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d1bd514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2d2a7975af42b38494c28e5965e209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name            |   accuracy |     f1 |    auc |\n",
      "|-----------------+------------+--------+--------|\n",
      "| phone_normal    |     0.7023 | 0.5208 | 0.6907 |\n",
      "| phone_biased    |     0.8026 | 0.3482 | 0.6036 |\n",
      "| email           |     0.7575 | 0.0401 | 0.5068 |\n",
      "| youtube         |     0.7582 | 0.0129 | 0.5024 |\n",
      "| home_phone      |     0.6146 | 0.5041 | 0.6808 |\n",
      "| site            |     0.7186 | 0.2075 | 0.5258 |\n",
      "| phone_operators |     0.7573 | 0.0166 | 0.5024 |\n"
     ]
    }
   ],
   "source": [
    "formatter = lambda x: '{:.4f}'.format(x)\n",
    "table = []\n",
    "\n",
    "for regexp_name in tqdm(regexps.keys()):\n",
    "    accuracy = accuracy_score(df_train['is_bad'], df_train[regexp_name])\n",
    "    f1 = f1_score(df_train['is_bad'], df_train[regexp_name])\n",
    "    auc = roc_auc_score(df_train['is_bad'], df_train[regexp_name])\n",
    "    table.append([regexp_name, formatter(accuracy), formatter(f1), formatter(auc)])\n",
    "\n",
    "print(tabulate(table, headers=['name', 'accuracy', 'f1', 'auc'], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b84d8d",
   "metadata": {},
   "source": [
    "Оставил те, у которых auc выше 0.5, номера и числа заменил на цифры, соцсети объединил и потестил ещё раз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c64f3",
   "metadata": {},
   "source": [
    "## Сохранение датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f480f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../data/train_preprocessed.csv', index=False)\n",
    "df_val.to_csv('../data/val_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdebdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
